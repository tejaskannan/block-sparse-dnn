{
    "name": "block_sparse_mlp",
    "num_epochs": 5,
    "hidden_units": [128, 128, 16],
    "block_size": 16,
    "sparsity": 0.17,
    "prune_fraction": 0.05,
    "hidden_activation": "relu",
    "dropout_keep_rate": 1.0,
    "l2": 0.01
}
